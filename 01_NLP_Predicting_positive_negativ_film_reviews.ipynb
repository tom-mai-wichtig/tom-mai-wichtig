{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tom-mai-wichtig/tom-mai-wichtig/blob/main/01_NLP_Predicting_positive_negativ_film_reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpyqfnn2zkD8",
        "outputId": "200aea24-580f-4da9-bf5b-c68ca385de79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fist reviews: bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
            "Second reviews: story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers . unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting . even those from the era should be turned off . the cryptic dialogue would make shakespeare seem easy to a third grader . on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond . future stars sally kirkland and frederic forrest can be seen briefly .  \n",
            "Lenth: 25000\n",
            "Last review: this is one of the dumbest films  i  ve ever seen . it rips off nearly ever type of thriller and manages to make a mess of them all .  br    br   there  s not a single good line or character in the whole mess . if there was a plot  it was an afterthought and as far as acting goes  there  s nothing good to say so ill say nothing . i honestly cant understand how this type of nonsense gets produced and actually released  does somebody somewhere not at some stage think   oh my god this really is a load of shite  and call it a day . its crap like this that has people downloading illegally  the trailer looks like a completely different film  at least if you have download it  you haven  t wasted your time or money don  t waste your time  this is painful .  \n",
            "Raw_reviews: ['bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   ', 'story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers . unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting . even those from the era should be turned off . the cryptic dialogue would make shakespeare seem easy to a third grader . on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond . future stars sally kirkland and frederic forrest can be seen briefly .  ']\n",
            "Lenth: 25000\n",
            "First labels: positive\n",
            "Secon labels: negative\n",
            "Last labels: negative\n"
          ]
        }
      ],
      "source": [
        "import base64\n",
        "import requests\n",
        "\n",
        "reviews = \"https://raw.githubusercontent.com/iamtrask/Grokking-Deep-Learning/master/reviews.txt\"\n",
        "raw_reviews = requests.get(reviews)\n",
        "raw_reviews = raw_reviews.text.split(\"\\n\")\n",
        "del raw_reviews[-1]\n",
        "print(f\"Fist reviews: {raw_reviews[0]}\")\n",
        "print(f\"Second reviews: {raw_reviews[1]}\")\n",
        "print(f\"Lenth: {len(raw_reviews)}\")\n",
        "print(f\"Last review: {raw_reviews[-1]}\")\n",
        "print(f\"Raw_reviews: {raw_reviews[:2]}\")\n",
        "\n",
        "labels = \"https://raw.githubusercontent.com/iamtrask/Grokking-Deep-Learning/master/labels.txt\"\n",
        "raw_labels = requests.get(labels)\n",
        "raw_labels = raw_labels.text\n",
        "raw_labels = raw_labels.split(\"\\n\")\n",
        "del raw_labels[-1]\n",
        "print(f\"Lenth: {len(raw_labels)}\")\n",
        "print(f\"First labels: {raw_labels[0]}\")\n",
        "print(f\"Secon labels: {raw_labels[1]}\")\n",
        "print(f\"Last labels: {raw_labels[-1]}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp_kTXZqoulm"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K288k8iu0Qul",
        "outputId": "7bd4646e-f571-48c0-c575-122210515ccf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[40523, 58168, 17040, 46908, 12254, 23260, 32530, 13, 71532, 73236, 40553, 36792, 21761, 53526, 52704, 54710, 40588, 5107, 23610, 4837, 21808, 54114, 52430, 69587, 42599, 20045, 63234, 54742, 73611, 53033, 68445, 40069, 13844, 15652, 60938, 32941, 25087, 33485, 37793, 38666, 69376, 52513, 58565, 67657, 23706, 73668, 69400, 28285, 21066, 63917, 30619, 38411, 4080, 40433, 30329, 50553, 46500, 43291, 73969, 45283, 61010, 53941, 16023, 26601, 33564, 61902, 6957, 44215, 69991, 11635, 20203, 14271, 46280, 62817, 62543, 73752, 54302, 15194, 57259, 67207, 65202, 68902, 12235, 58707, 66920, 22347, 12555, 824, 3892, 20872, 3623, 11676]\n",
            "Lenth of first sentence: 92\n",
            "112\n",
            "['positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative']\n",
            "[1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n"
          ]
        }
      ],
      "source": [
        "# spliting words in reviews\n",
        "#token = raw_reviews[0]\n",
        "#token = token.split(\" \")\n",
        "#print(len(token))\n",
        "#token = set(token)\n",
        "#print(len(token))\n",
        "#print(token)\n",
        "tokens = list(map(lambda x: set(x.split(\" \")), raw_reviews))\n",
        "\n",
        "#print(f\"\\n{token == tokens[0]}\")\n",
        "\n",
        "# list of vocabulary\n",
        "def vocab_list(tokens):\n",
        "    \"\"\"\n",
        "    Input: tokens\n",
        "    ex: today I ate two two Hamburger\n",
        "    Output: vocab list\n",
        "    ex: ['today', 'I', 'ate', 'two', 'Hamburger']\n",
        "    \"\"\"\n",
        "    vocab = set()\n",
        "    for sent in tokens:\n",
        "        for word in sent:\n",
        "            if (len(word)>0):\n",
        "                vocab.add(word)\n",
        "    return list(vocab)\n",
        "\n",
        "vocab = vocab_list(tokens)\n",
        "\n",
        "def converting_vocab_to_index(vocab):\n",
        "    \"\"\"\n",
        "    Input: vocab\n",
        "    ex: ['today', 'I', 'ate', 'two', 'Hamburger']\n",
        "    Output: word2index\n",
        "    ex: {'today': 0, 'I': 1, 'ate': 2, 'two': 3, 'Hamburger': 4}\n",
        "    \"\"\"\n",
        "    word2index = {}\n",
        "    for i, word in enumerate(vocab):\n",
        "        word2index[word] = i\n",
        "    return word2index\n",
        "\n",
        "word2index = converting_vocab_to_index(vocab)\n",
        "\n",
        "# building dataframe\n",
        "def converting_sent_to_list_number(tokens):\n",
        "    input_dataset = list()\n",
        "    for sent in tokens:\n",
        "        sent_in_number_form = list()\n",
        "        for word in sent:\n",
        "            if (len(word)>0):\n",
        "                sent_in_number_form.append(word2index[word])\n",
        "        input_dataset.append(sent_in_number_form)\n",
        "    return input_dataset\n",
        "\n",
        "input_dataset = converting_sent_to_list_number(tokens)\n",
        "print(input_dataset[0])\n",
        "\n",
        "print(f\"Lenth of first sentence: {len(input_dataset[0])}\")\n",
        "print(len(input_dataset[15]))\n",
        "\n",
        "def one_hot_labeling (raw_labels):\n",
        "    target_dataset = list()\n",
        "    for label in raw_labels:\n",
        "        if label == \"positive\":\n",
        "            target_dataset.append(1)\n",
        "        else:\n",
        "            target_dataset.append(0)\n",
        "    return target_dataset\n",
        "\n",
        "target_dataset = one_hot_labeling(raw_labels)\n",
        "print(raw_labels[:10])\n",
        "print(target_dataset[:10])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7ONdB5TPHJN",
        "outputId": "183123a6-9153-4560-b88c-9eec9c95b9a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40523\n",
            "58168\n",
            "17040\n",
            "46908\n",
            "12254\n",
            "23260\n",
            "32530\n",
            "13\n",
            "71532\n",
            "73236\n",
            "40553\n",
            "36792\n",
            "21761\n",
            "53526\n",
            "52704\n",
            "54710\n",
            "40588\n",
            "5107\n",
            "23610\n",
            "4837\n",
            "21808\n",
            "54114\n",
            "52430\n",
            "69587\n",
            "42599\n",
            "20045\n",
            "63234\n",
            "54742\n",
            "73611\n",
            "53033\n",
            "68445\n",
            "40069\n",
            "13844\n",
            "15652\n",
            "60938\n",
            "32941\n",
            "25087\n",
            "33485\n",
            "37793\n",
            "38666\n",
            "69376\n",
            "52513\n",
            "58565\n",
            "67657\n",
            "23706\n",
            "73668\n",
            "69400\n",
            "28285\n",
            "21066\n",
            "63917\n",
            "30619\n",
            "38411\n",
            "4080\n",
            "40433\n",
            "30329\n",
            "50553\n",
            "46500\n",
            "43291\n",
            "73969\n",
            "45283\n",
            "61010\n",
            "53941\n",
            "16023\n",
            "26601\n",
            "33564\n",
            "61902\n",
            "6957\n",
            "44215\n",
            "69991\n",
            "11635\n",
            "20203\n",
            "14271\n",
            "46280\n",
            "62817\n",
            "62543\n",
            "73752\n",
            "54302\n",
            "15194\n",
            "57259\n",
            "67207\n",
            "65202\n",
            "68902\n",
            "12235\n",
            "58707\n",
            "66920\n",
            "22347\n",
            "12555\n",
            "824\n",
            "3892\n",
            "20872\n",
            "3623\n",
            "11676\n",
            "1\n",
            "{'', 'i', 'at', 'can', 'and', 'some', 'here', 'pathetic', 'adults', 'than', 'fetched', 'a', 'scramble', 'students', 'believe', 'ran', 'your', 'far', 'what', 'sack', 'cartoon', 'their', 'is', 'comedy', 'one', 'schools', 'pity', 'age', '.', 'reality', 'of', 'profession', 'burn', 'teaching', 'satire', 'same', 'all', 'see', 'lead', 'bromwell', 'welcome', 'me', 'it', 'situation', 'the', 'isn', 'recalled', 'episode', 'my', 'through', 'classic', 'when', 'knew', 'pomp', 'immediately', 'programs', 'to', 'who', 't', 'survive', 'repeatedly', 'student', 'whole', 'down', 'which', 'years', 'insightful', 'teachers', 'pettiness', 'remind', 'financially', 'other', 'expect', 'right', 'in', 'tried', 'such', 'life', 'm', 's', 'time', 'line', 'saw', 'inspector', 'closer', 'much', 'about', 'think', 'as', 'that', 'many', 'school', 'high'}\n",
            "[40523, 58168, 17040, 46908, 12254, 23260, 32530, 13, 71532, 73236, 40553, 36792, 21761, 53526, 52704, 54710, 40588, 5107, 23610, 4837, 21808, 54114, 52430, 69587, 42599, 20045, 63234, 54742, 73611, 53033, 68445, 40069, 13844, 15652, 60938, 32941, 25087, 33485, 37793, 38666, 69376, 52513, 58565, 67657, 23706, 73668, 69400, 28285, 21066, 63917, 30619, 38411, 4080, 40433, 30329, 50553, 46500, 43291, 73969, 45283, 61010, 53941, 16023, 26601, 33564, 61902, 6957, 44215, 69991, 11635, 20203, 14271, 46280, 62817, 62543, 73752, 54302, 15194, 57259, 67207, 65202, 68902, 12235, 58707, 66920, 22347, 12555, 824, 3892, 20872, 3623, 11676]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def check_input_dataset_with_tokens(int):\n",
        "    \"\"\"\n",
        "    Input: integer as index for sentence in both input_dataset and tokens set\n",
        "    Output: check if given line correct in both input_datset and tokens set\n",
        "     \"\"\"\n",
        "    for word in input_dataset[int]:\n",
        "        if word in tokens[int]:\n",
        "            continue\n",
        "        else:\n",
        "            print(word)\n",
        "    print(len(tokens[int]) - len(input_dataset[int]))\n",
        "\n",
        "check_input_dataset_with_tokens(0)\n",
        "print(tokens[0])\n",
        "print(input_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ERhHzjS-0a6f"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yyp1cXc4o_6m"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1)\n",
        "def sigmoid(x):\n",
        "    return (1/(1+ np.exp(-x)))\n",
        "def deri2sigmoid(out):\n",
        "    return out *(1-out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WAg8X0tfcBKl"
      },
      "outputs": [],
      "source": [
        "alpha, iterations = (0.01,50)\n",
        "hidden_size=100\n",
        "\n",
        "w01 = 0.2* np.random.random((len(vocab), hidden_size))-.1\n",
        "w12 = 0.2* np.random.random((hidden_size, 1))-0.1\n",
        "#x = input_dataset[0]\n",
        "#li = [0,1,2]\n",
        "#print(len(li))\n",
        "#a = np.random.randint(0,10, size = (len(li),5))\n",
        "#print(a)\n",
        "#print(a[li])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3BvTBa_cuDf",
        "outputId": "8aea7dd3-aa07-4c69-b8ef-2845245c3317"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rIter: 0\n",
            "Progress: 0.9999583333333333\n",
            "Train_acc: 0.6781666666666667\n",
            "Test_acc 0.82\n",
            "Iter: 10\n",
            "Progress: 0.9999583333333333\n",
            "Train_acc: 0.9243333333333333\n",
            "Test_acc 0.862\n",
            "Iter: 20\n",
            "Progress: 0.9999583333333333\n",
            "Train_acc: 0.9549166666666666\n",
            "Test_acc 0.853\n",
            "Iter: 30\n",
            "Progress: 0.9999583333333333\n",
            "Train_acc: 0.9731666666666666\n",
            "Test_acc 0.85\n",
            "Iter: 40\n",
            "Progress: 0.9999583333333333\n",
            "Train_acc: 0.9813333333333333\n",
            "Test_acc 0.854\n",
            "Iter: 50\n",
            "Progress: 0.9999583333333333\n",
            "Train_acc: 0.986\n",
            "Test_acc 0.853\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for iter in range(iterations+1):\n",
        "    correct, total = (0,0)\n",
        "    for _ in range (len(input_dataset)-1000):\n",
        "        x, y = (input_dataset[_], target_dataset[_])\n",
        "        l_1 = sigmoid(np.sum(w01[x],axis=0))\n",
        "        drop_mask = np.random.randint(2, size = l_1.shape)\n",
        "        l_1*= drop_mask\n",
        "        l_2 = sigmoid (np.dot(l_1, w12))\n",
        "\n",
        "        del_2 = l_2 - y\n",
        "        del_1 = w12.dot(del_2)*deri2sigmoid(l_1)* drop_mask\n",
        "\n",
        "        w01[x] -=del_1 * alpha\n",
        "        w12 -= np.outer(l_1, del_2)*alpha\n",
        "\n",
        "        if (np.abs(l_2-y) <0.5):\n",
        "            correct += 1\n",
        "        total +=1\n",
        "    if (iter %10==0):\n",
        "        progress = _/float(len(input_dataset)-1000)\n",
        "        print(f\"\\rIter: {iter}\")\n",
        "        print(f\"Progress: {progress}\")\n",
        "        print(f\"Train_acc: {correct/total}\")\n",
        "\n",
        "        correct_test, total_test = (0,0)\n",
        "        for i in range( len(input_dataset)-1000, len(input_dataset)):\n",
        "            x = input_dataset[i]\n",
        "            y = target_dataset[i]\n",
        "\n",
        "            l_1 = sigmoid(np.sum(w01[x], axis=0))\n",
        "            l_2 = sigmoid(np.dot(l_1, w12))\n",
        "            if (np.abs(l_2 - y) <0.5):\n",
        "                correct_test+=1\n",
        "            total_test+=1\n",
        "        print(f\"Test_acc {correct_test/total_test}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5u-8WTB3PSF"
      },
      "source": [
        "Comparing word embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kUHMpn3Rr4to"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jtDgEdrP3ZX5"
      },
      "outputs": [],
      "source": [
        "def similar(target='beautiful'):\n",
        "    target_index = word2index[target]\n",
        "    scores = Counter()\n",
        "    for word, index in word2index.items():\n",
        "        raw_diff = w01[index] - w01[target_index]\n",
        "        squarred_dif = raw_diff**2\n",
        "        scores[word] = -math.sqrt(sum(squarred_dif))\n",
        "    return scores.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJmPuxKB5UiK",
        "outputId": "23de7a67-796c-4974-a66f-50e88ffadee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('beautiful', -0.0), ('compelling', -1.0104209836342637), ('notting', -1.0340515548316989), ('glued', -1.0576902320241088), ('timing', -1.058087111264678), ('essential', -1.0650764042532694), ('bud', -1.0673712648649756), ('surpasses', -1.070822394675594), ('corner', -1.0712781634730935), ('elisha', -1.073877943750883)]\n"
          ]
        }
      ],
      "source": [
        "print(similar('beautiful'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_bSn_RD6DG_",
        "outputId": "aae01077-010a-45c9-a53f-fe869254b03c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('terrible', -0.0), ('horrible', -1.086224047790112), ('dull', -1.1129429268464572), ('poor', -1.1406123672503858), ('awful', -1.1883432037778172), ('disappointing', -1.1939214462412724), ('lacks', -1.1965547092199258), ('disappointment', -1.1994503328734716), ('avoid', -1.2140697062678836), ('laughable', -1.219016803817461)]\n"
          ]
        }
      ],
      "source": [
        "print(similar('terrible'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vDLUGlmV6b03"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOPcBLU/+7w0OL1+fEuBzhp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}